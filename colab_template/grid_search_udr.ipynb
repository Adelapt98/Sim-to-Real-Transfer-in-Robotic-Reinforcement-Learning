{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siz3Hiq1JUZ1"
      },
      "source": [
        "Install and load all dependencies (first time only) \\\n",
        "NOTE: you may need to restart the runtime afterwards (CTRL+M .)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW6XT0jSJI8e"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "\n",
        "!pip install gym\n",
        "!pip install free-mujoco-py\n",
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwIRXGd5K3xJ"
      },
      "source": [
        "Set up the custom Hopper environment\n",
        "\n",
        "\n",
        "\n",
        "1.   Upload `classes.zip` to the current session's file storage\n",
        "2.   Un-zip it by running cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9WsofDVLaCC"
      },
      "outputs": [],
      "source": [
        "!unzip classes.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pJC_JevLf1f"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4NsuF6pJPVJ"
      },
      "source": [
        "\\\n",
        "\n",
        "**Train an RL agent on the OpenAI Gym Hopper environment using REINFORCE and Actor-critic algorithms**\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "TASK 2 and 3: interleave data collection to policy updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTYmUufrJTNl"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import gym\n",
        "from stable_baselines3 import PPO, SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from env.custom_hopper_udrr import CustomHopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the parameter grid\n",
        "scale_min_values = [0.7, 0.8, 0.9]\n",
        "scale_max_values = [1.1, 1.2, 1.3]\n",
        "ranges = {1:[[0, 0]], 2:[0, 0], 3:[0, 0]}\n",
        "total_timesteps = 100000  # Adjust based on your requirements\n",
        "\n",
        "# Initialize results storage\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define environment with the current parameters\n",
        "def train_and_evaluate(scale_min, scale_max):\n",
        "    env_id = 'CustomHopper-source-v0'\n",
        "    env = gym.make(env_id)\n",
        "    env.unwrapped.scale_min = scale_min\n",
        "    env.unwrapped.scale_max = scale_max\n",
        "\n",
        "    model = PPO('MlpPolicy', env, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "    \n",
        "    return mean_reward, std_reward\n",
        "\n",
        "# Perform grid search\n",
        "for scale_min, scale_max in itertools.product(scale_min_values, scale_max_values):\n",
        "    mean_reward, std_reward = train_and_evaluate(scale_min, scale_max)\n",
        "    results.append((scale_min, scale_max, mean_reward, std_reward))\n",
        "    print(f\"Scale Min: {scale_min}, Scale Max: {scale_max}, Mean Reward: {mean_reward}, Std Reward: {std_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
